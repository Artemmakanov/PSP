{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting requests~=2.32.0 (from arxiv)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests~=2.32.0->arxiv)\n",
      "  Downloading charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests~=2.32.0->arxiv)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests~=2.32.0->arxiv)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests~=2.32.0->arxiv)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=1d4f6e15c76dc73793feb3f42c965bc832af742be1925f773e6d8f10096e254b\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, urllib3, idna, feedparser, charset-normalizer, certifi, requests, arxiv\n",
      "Successfully installed arxiv-2.1.3 certifi-2024.8.30 charset-normalizer-3.4.0 feedparser-6.0.11 idna-3.10 requests-2.32.3 sgmllib3k-1.0.0 urllib3-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as libreq\n",
    "import xmltodict, json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3Arecsys%26id_list%3D%26start%3D0%26max_results%3D2\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=all:recsys&amp;id_list=&amp;start=0&amp;max_results=2</title>\\n  <id>http://arxiv.org/api/an+Fj2R0gQtu5A44l3AaQmhOvK8</id>\\n  <updated>2024-11-19T00:00:00-05:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">466</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">2</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2207.08713v1</id>\\n    <updated>2022-07-13T12:13:00Z</updated>\\n    <published>2022-07-13T12:13:00Z</published>\\n    <title>The Impact of Feature Quantity on Recommendation Algorithm Performance:\\n  A Movielens-100K Case Study</title>\\n    <summary>  Recent model-based Recommender Systems (RecSys) algorithms emphasize on the\\nuse of features, also called side information, in their design similar to\\nalgorithms in Machine Learning (ML). In contrast, some of the most popular and\\ntraditional algorithms for RecSys solely focus on a given user-item-rating\\nrelation without including side information. The goal of this case study is to\\nprovide a performance comparison and assessment of RecSys and ML algorithms\\nwhen side information is included. We chose the Movielens-100K data set since\\nit is a standard for comparing RecSys algorithms. We compared six different\\nfeature sets with varying quantities of features which were generated from the\\nbaseline data and evaluated on a total of 19 RecSys algorithms, baseline ML\\nalgorithms, Automated Machine Learning (AutoML) pipelines, and state-of-the-art\\nRecSys algorithms that incorporate side information. The results show that\\nadditional features benefit all algorithms we evaluated. However, the\\ncorrelation between feature quantity and performance is not monotonous for\\nAutoML and RecSys. In these categories, an analysis of feature importance\\nrevealed that the quality of features matters more than quantity. Throughout\\nour experiments, the average performance on the feature set with the lowest\\nnumber of features is about 6% worse compared to that with the highest in terms\\nof the Root Mean Squared Error. An interesting observation is that AutoML\\noutperforms matrix factorization-based RecSys algorithms when additional\\nfeatures are used. Almost all algorithms that can include side information have\\nhigher performance when using the highest quantity of features. In the other\\ncases, the performance difference is negligible (&lt;1%). The results show a clear\\npositive trend for the effect of feature quantity as well as the important\\neffects of feature quality on the evaluated algorithms.\\n</summary>\\n    <author>\\n      <name>Lukas Wegmeth</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2207.08713v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2207.08713v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/2402.04453v1</id>\\n    <updated>2024-02-06T22:42:28Z</updated>\\n    <published>2024-02-06T22:42:28Z</published>\\n    <title>The Potential of AutoML for Recommender Systems</title>\\n    <summary>  Automated Machine Learning (AutoML) has greatly advanced applications of\\nMachine Learning (ML) including model compression, machine translation, and\\ncomputer vision. Recommender Systems (RecSys) can be seen as an application of\\nML. Yet, AutoML has found little attention in the RecSys community; nor has\\nRecSys found notable attention in the AutoML community. Only few and relatively\\nsimple Automated Recommender Systems (AutoRecSys) libraries exist that adopt\\nAutoML techniques. However, these libraries are based on student projects and\\ndo not offer the features and thorough development of AutoML libraries. We set\\nout to determine how AutoML libraries perform in the scenario of an\\ninexperienced user who wants to implement a recommender system. We compared the\\npredictive performance of 60 AutoML, AutoRecSys, ML, and RecSys algorithms from\\n15 libraries, including a mean predictor baseline, on 14 explicit feedback\\nRecSys datasets. To simulate the perspective of an inexperienced user, the\\nalgorithms were evaluated with default hyperparameters. We found that AutoML\\nand AutoRecSys libraries performed best. AutoML libraries performed best for\\nsix of the 14 datasets (43%), but it was not always the same AutoML library\\nperforming best. The single-best library was the AutoRecSys library\\nAuto-Surprise, which performed best on five datasets (36%). On three datasets\\n(21%), AutoML libraries performed poorly, and RecSys libraries with default\\nparameters performed best. Although, while obtaining 50% of all placements in\\nthe top five per dataset, RecSys algorithms fall behind AutoML on average. ML\\nalgorithms generally performed the worst.\\n</summary>\\n    <author>\\n      <name>Tobias Vente</name>\\n    </author>\\n    <author>\\n      <name>Joeran Beel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2402.04453v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.04453v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with libreq.urlopen('http://export.arxiv.org/api/query?search_query=all:recsys&start=0&max_results=2&sortBy=relevance&sortOrder=descending') as url:\n",
    "    r = url.read()\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = xmltodict.parse(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper:\n",
    "    def __init__(self, url, title, author, published):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.published = published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "search = arxiv.Search(\n",
    "  query = \"recsys\",\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.Relevance\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "for paper_arxiv in client.results(search):\n",
    "    # Download the PDF to a specified directory with a custom filename.\n",
    "    try:\n",
    "        paper_arxiv.download_pdf(dirpath=\"/home/ubuntu/Documents/PSP/dump\", ) #filename=\"downloaded-paper.pdf\")\n",
    "        papers.append(Paper(paper_arxiv.entry_id,  paper_arxiv.title, paper_arxiv.authors, paper_arxiv.published,))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.24.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P. Romov and E. Sokolov. Recsys challenge 2015: Ensemble learning with categorical features. In Proceedings\n",
      "T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In B. Krishnapuram, M. Shah, A. J.\n",
      "M. Volkovs. Two-stage approach to item recommendation from user sessions. In Proceedings of the 2015\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Iterate through each page and extract text\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "pdf_path = '/home/ubuntu/Documents/PSP/dump/1612.00959v1.RecSys_Challenge_2016__job_recommendations_based_on_preselection_of_offers_and_gradient_boosting.pdf'  # Replace with your PDF file path\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "\n",
    "\n",
    "def find_references_section(text):\n",
    "    # Find the start of the references section\n",
    "    references_start = text.lower().find(\"references\")\n",
    "    if references_start == -1:\n",
    "        references_start = text.lower().find(\"bibliography\")\n",
    "    \n",
    "    if references_start == -1:\n",
    "        raise ValueError(\"References section not found\")\n",
    "    \n",
    "    # Extract the references section\n",
    "    references_text = text[references_start:]\n",
    "    return references_text\n",
    "\n",
    "references_text = find_references_section(text)\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_paper_names(references_text):\n",
    "    # Regular expression to match the start of a reference\n",
    "    reference_pattern = re.compile(r'^\\s*\\[\\d+\\]\\s+(.*)', re.MULTILINE)\n",
    "    \n",
    "    # Find all matches\n",
    "    matches = reference_pattern.findall(references_text)\n",
    "    \n",
    "    # Remove duplicates by converting to a set, then back to a list\n",
    "    unique_paper_names = list(set(matches))\n",
    "    \n",
    "    return unique_paper_names\n",
    "\n",
    "paper_names = extract_paper_names(references_text)\n",
    "for name in paper_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P. Romov and E. Sokolov. Recsys challenge 2015: Ensemble learning with categorical features. In Proceedings', 'T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In B. Krishnapuram, M. Shah, A. J.', 'M. Volkovs. Two-stage approach to item recommendation from user sessions. In Proceedings of the 2015']\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
