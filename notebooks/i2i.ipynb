{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# import these modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper:\n",
    "    def __init__(self, url, title, author, published, refs):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.published = published\n",
    "        self.refs = refs\n",
    "    \n",
    "with open('/home/admin/PSP/backend/pdfs/papers_stucture_recommender.pickle', 'rb') as handle:\n",
    "    papers = pickle.load(handle)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperWithStems(Paper):\n",
    "    def __init__(self, p):\n",
    "        super().__init__(p.url, p.title, p.author, p.published, p.refs)\n",
    "        self.stems = set(ps.stem(w) for w in p.title.split())\n",
    "        self.refs = [\n",
    "            set(ps.stem(w) for w in ref.split())\n",
    "            for ref in p.refs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 1000/1000 [00:56<00:00, 17.63it/s]\n"
     ]
    }
   ],
   "source": [
    "papers_with_stems = [PaperWithStems(p) for p in tqdm(papers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def similar(stems, refs):\n",
    "#     for ref in refs:\n",
    "#         if len(stems & ref) / len(stems) > 0.98:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "def similar(stems, refs):\n",
    "    for ref in refs:\n",
    "        if len(stems & ref) == len(stems):\n",
    "            return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "def build_similarity_matrix(papers):\n",
    "    \"\"\"Создает матрицу похожести статей.\"\"\"\n",
    "    n = len(papers)\n",
    "    matrix = np.zeros((n, n), dtype=np.float32)\n",
    "    \n",
    "    for i in tqdm(range(n)):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                if similar(papers[i].stems, papers[j].refs):\n",
    "                    matrix[i, j] = 1\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def filter_empty_rows_and_cols(matrix, papers):\n",
    "    \"\"\"Удаляет пустые строки и столбцы из матрицы.\"\"\"\n",
    "    non_zero_rows = np.any(matrix > 0, axis=1)\n",
    "    non_zero_cols = np.any(matrix > 0, axis=0)\n",
    "    filtered_matrix = matrix[np.ix_(non_zero_rows, non_zero_cols)]\n",
    "    filtered_papers = [p for i, p in enumerate(papers) if non_zero_rows[i]]\n",
    "    paperid2filtered_paperid = {\n",
    "        paperid: filtered_papers.index(paper) \n",
    "        for paperid, paper in enumerate(papers)\n",
    "        if paper in filtered_papers}\n",
    "\n",
    "    return filtered_matrix, filtered_papers, paperid2filtered_paperid\n",
    "\n",
    "def build_annoy_index(matrix, num_trees=10):\n",
    "    \"\"\"Строит Annoy-индекс на основе SVD-разложения матрицы.\"\"\"\n",
    "    u, s, vt = svds(matrix, k=min(30, min(matrix.shape)-1))  # SVD\n",
    "    embeddings = u @ np.diag(s)  # Проекция на уменьшенное пространство\n",
    "    \n",
    "    dim = embeddings.shape[1]\n",
    "    annoy_index = AnnoyIndex(dim, metric='euclidean')\n",
    "    \n",
    "    for i, vec in enumerate(embeddings):\n",
    "        annoy_index.add_item(i, vec)\n",
    "    \n",
    "    annoy_index.build(num_trees)\n",
    "    return annoy_index\n",
    "\n",
    "def get_similar_articles(annoy_index, article_index, top_n=5):\n",
    "    \"\"\"Получает индексы похожих статей из Annoy-индекса.\"\"\"\n",
    "    return annoy_index.get_nns_by_item(article_index, 1 + top_n)[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования\n",
    "similarity_matrix = build_similarity_matrix(papers_with_stems)\n",
    "similarity_matrix.sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_matrix, filtered_articles, paperid2filtered_paperid = filter_empty_rows_and_cols(similarity_matrix, papers_with_stems)\n",
    "filtered_paper2idpaperid = {\n",
    "    filtered_paperid: paperid for\n",
    "    paperid, filtered_paperid in paperid2filtered_paperid.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_index = build_annoy_index(filtered_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annoy_index.save('papers.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Factorization Machines for Cross-Domain Recommendation\n",
      "==========================================================================================\n",
      "Похожие статьи: The topological face of recommendation: models and application to bias detection\n",
      "Large Language Models as Recommender Systems: A Study of Popularity Bias\n",
      "Cross-domain recommendation via user interest alignment\n",
      "Improving Rating and Relevance with Point-of-Interest Recommender System\n",
      "Distilling Structured Knowledge into Embeddings for Explainable and Accurate Recommendation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Получаем похожие статьи для первой статьи в отфильтрованном списке\n",
    "paperid = 101\n",
    "filtered_paperid = paperid2filtered_paperid[paperid]\n",
    "\n",
    "print(filtered_articles[filtered_paperid].title)\n",
    "similar_indices = get_similar_articles(annoy_index, filtered_paperid)\n",
    "print('='*90)\n",
    "print(\"Похожие статьи:\", \"\\n\".join([filtered_articles[i].title for i in similar_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Похожие статьи: The topological face of recommendation: models and application to bias detection\n",
      "Large Language Models as Recommender Systems: A Study of Popularity Bias\n",
      "Cross-domain recommendation via user interest alignment\n",
      "Improving Rating and Relevance with Point-of-Interest Recommender System\n",
      "Distilling Structured Knowledge into Embeddings for Explainable and Accurate Recommendation\n"
     ]
    }
   ],
   "source": [
    "print(\"Похожие статьи:\", \"\\n\".join([papers_with_stems[filtered_paper2idpaperid[i]].title for i in similar_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/admin/PSP/backend/pdfs/filtered_paper2idpaperid.pickle', 'wb') as handle:\n",
    "    pickle.dump(filtered_paper2idpaperid, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Overhead-free User-side Recommender Systems'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_with_stems[0].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.url = url\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.published = published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "def generate_citation_link(title: str, authors: list):\n",
    "    formatted_authors = authors\n",
    "    citation = f\"{formatted_authors}. {title}.\"\n",
    "    \n",
    "    return f\"https://example.com/citation?{urllib.parse.urlencode({'cite': citation})}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://example.com/citation?cite=%5Barxiv.Result.Author%28%27Ryoma+Sato%27%29%5D.+Overhead-free+User-side+Recommender+Systems.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_citation_link(papers_with_stems[0].title, papers_with_stems[0].author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_with_stems[0].published.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kexin Yin et al. 2022. Overhead-free User-side Recommender Systems'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{str(papers_with_stems[1].author[0])} et al. {papers_with_stems[1].published.year}. {papers_with_stems[0].title}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    {\n",
    "        'author': str(paper.author[0]),\n",
    "        'year': paper.published.year,\n",
    "        'title': paper.title,\n",
    "        'stems': paper.stems\n",
    "    }\n",
    "    for paper in papers_with_stems\n",
    "]\n",
    "\n",
    "len(papers)\n",
    "# with open('/home/admin/PSP/backend/pdfs/papers.pickle', 'wb') as handle:\n",
    "#     pickle.dump(papers, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Ryoma Sato',\n",
       " 'year': 2024,\n",
       " 'title': 'Overhead-free User-side Recommender Systems',\n",
       " 'stems': {'overhead-fre', 'recommend', 'system', 'user-sid'}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PapersHandler:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        with open('/home/admin/PSP/backend/pdfs/filtered_paper2idpaperid.pickle', 'rb') as handle:\n",
    "            self.filtered_paper2idpaperid = pickle.load(handle)\n",
    "        with open('/home/admin/PSP/backend/pdfs/paperid2filtered_paperid.pickle', 'rb') as handle:\n",
    "            self.paperid2filtered_paperid = pickle.load(handle)\n",
    "        with open('/home/admin/PSP/backend/pdfs/papers.pickle', 'rb') as handle:\n",
    "            self.papers = pickle.load(handle)\n",
    "\n",
    "        f = 30\n",
    "        self.annoy_index = AnnoyIndex(f, 'euclidean')\n",
    "        self.annoy_index.load('/home/admin/PSP/backend/pdfs/papers.ann') # super fast, will just mmap the file\n",
    "    \n",
    "    def get_similar_articles(self, paper_id, top_n=5):\n",
    "\n",
    "        filtered_paperid = self.paperid2filtered_paperid[paper_id]\n",
    "        filtered_paperids = self.annoy_index.get_nns_by_item(filtered_paperid, 1 + top_n)[1:]\n",
    "        paperids = [\n",
    "            self.filtered_paper2idpaperid[filtered_paperid] \n",
    "            for filtered_paperid in filtered_paperids]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                'id': paperid,\n",
    "                'title': self.papers[paperid]['title'],\n",
    "                'link': self.generate_citation_link(self.papers[paperid])\n",
    "            }\n",
    "            for paperid in paperids\n",
    "        ]\n",
    "        \n",
    "    def search(self, query: str):\n",
    "        query_stems = set(ps.stem(w) for w in query.split())\n",
    "        results = []\n",
    "        for paperid, filtered_paperid in paperid2filtered_paperid.items():\n",
    "            paper = self.papers[paperid]\n",
    "            if len(paper['stems'] & query_stems)> 0:\n",
    "                results.append({\n",
    "                    'id': paperid,\n",
    "                    'title': paper['title']\n",
    "                })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def generate_pdf_url(self, id):\n",
    "        return f'http://localhost:5000/article/{id}'\n",
    "\n",
    "    def generate_citation_link(self, paper):\n",
    "        return f\"{paper['author']} et al. {paper['year']}. {paper['title']}\"\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 19, 'title': 'A Survey on the Fairness of Recommender Systems'},\n",
       " {'id': 23,\n",
       "  'title': 'Multi-stakeholder Recommendation and its Connection to Multi-sided Fairness'},\n",
       " {'id': 108,\n",
       "  'title': 'The Connection Between Popularity Bias, Calibration, and Fairness in Recommendation'},\n",
       " {'id': 121,\n",
       "  'title': 'Towards Communication Efficient and Fair Federated Personalized Sequential Recommendation'},\n",
       " {'id': 148,\n",
       "  'title': 'Personalized Counterfactual Fairness in Recommendation'},\n",
       " {'id': 157, 'title': 'Fairness of Exposure in Dynamic Recommendation'},\n",
       " {'id': 179, 'title': 'Towards Fair Conversational Recommender Systems'},\n",
       " {'id': 194, 'title': 'Towards Long-term Fairness in Recommendation'},\n",
       " {'id': 214,\n",
       "  'title': 'Transparency, Privacy, and Fairness in Recommender Systems'},\n",
       " {'id': 259,\n",
       "  'title': 'The Impact of Popularity Bias on Fairness and Calibration in Recommendation'},\n",
       " {'id': 262,\n",
       "  'title': 'A General Framework for Fairness in Multistakeholder Recommendations'},\n",
       " {'id': 268, 'title': 'User-oriented Fairness in Recommendation'},\n",
       " {'id': 269,\n",
       "  'title': 'Private Recommender Systems: How Can Users Build Their Own Fair Recommender Systems without Log Data?'},\n",
       " {'id': 270, 'title': 'Enumerating Fair Packages for Group Recommendations'},\n",
       " {'id': 342, 'title': 'Explainable Fairness in Recommendation'},\n",
       " {'id': 347, 'title': 'Transferable Fairness for Cold-Start Recommendation'},\n",
       " {'id': 444,\n",
       "  'title': 'Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning'},\n",
       " {'id': 451,\n",
       "  'title': 'Cali3F: Calibrated Fast Fair Federated Recommendation System'},\n",
       " {'id': 519,\n",
       "  'title': 'DifFaiRec: Generative Fair Recommender with Conditional Diffusion Model'},\n",
       " {'id': 522, 'title': 'User-item fairness tradeoffs in recommendations'},\n",
       " {'id': 534, 'title': 'User Fairness in Recommender Systems'},\n",
       " {'id': 536, 'title': 'Multisided Fairness for Recommendation'},\n",
       " {'id': 540,\n",
       "  'title': 'Exploring User Opinions of Fairness in Recommender Systems'},\n",
       " {'id': 606,\n",
       "  'title': 'Equality of Learning Opportunity via Individual Fairness in Personalized Recommendations'},\n",
       " {'id': 630,\n",
       "  'title': 'Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation'},\n",
       " {'id': 777,\n",
       "  'title': 'Long-term Dynamics of Fairness Intervention in Connection Recommender Systems'},\n",
       " {'id': 812,\n",
       "  'title': 'Counterfactual Explanation for Fairness in Recommendation'},\n",
       " {'id': 866, 'title': 'Fair Reciprocal Recommendation in Matching Markets'},\n",
       " {'id': 945, 'title': 'User-item matching for recommendation fairness'},\n",
       " {'id': 981,\n",
       "  'title': 'Experiments on Generalizability of User-Oriented Fairness in Recommender Systems'},\n",
       " {'id': 983, 'title': 'Comprehensive Fair Meta-learned Recommender System'}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph = PapersHandler()\n",
    "ph.search('fair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/admin/PSP/backend/pdfs/ph.pickle', 'wb') as handle:\n",
    "#     pickle.dump(ph, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
